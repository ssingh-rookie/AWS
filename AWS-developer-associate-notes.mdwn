# Notes for AWS Developer Associate Exam...

**Date started**: 14th Sep 2020

## 1. AWS Fundamentals

#### Scalability
* **Vertical scalability** - increasing the size of the application 
* **Horizontal Scalability** - Increasing the size of no. of instances / applications


#### Load Balancers 
- Network load balancer
- Application Load Balancer
- Classic Load Balancer



* Managed Service
  * Load balancers are managed, high availability, AWS guarantees it will work
  * AWS takes care of upgrades
  * AWS provides very few config options for clients to tinker


* Costs less to set up but more effort on clients end
* Integrated with many AWS offerings and services

## 6. RDS

#### Elasticache 

* Same as RDS, in memory cache, high performance, low latency
* Write scaling using sharding
* Read scaling using read replicas
* Multi-AZ with fail over capability
* AWS take care of OS maintenence, patching, optimizations, setup, config, failure recovery and backups

#### How does elastic cache fit into architecture

* Apps hit the cache before querying data in the database
* if it finds what its looking for - its **cache hit**
* if not, its a **cache miss** and it goes looks for in the database or RDS
* the app is programmed to write into cache for subsequent cache reads

Great use case of writing user session in a web app where apps are stateless

**Exam tip** - Typical use cases are user session management, compute heavy read queries, and offload db reads. Cannot be accessed from any other VPC, cannot provision more EC2 compute as its fully managed. Redis can run multiple modes (cluster enabled bs disabled) 

Cluster in redis activates sharding, multiple shards can have replicas in different AZ


#### Elasticache - Redis vs Memcached

REDIS | MEMCACHED
------|----------
multi-AZ with auto failover | multi - node for partitioning of data (sharding)
read replicas to scale reads and have high availability | Non persistent 
Data durability using AOF persistence (before after restart of cache) | No backup and restore
Backup and restore features | Multi-threaded architecture

redis - think RDS
memcache - think shards, distributed storing system

https://aws.amazon.com/caching/best-practices/

#### Caching impmentation considerations
* check if its safe to cache data
* is caching effective for data
  * pattern - data changing slowly, few keys frequently needed
  * anti-pattern: data changing rapidly, all large key space frequently needed

* is data well structured for caching?
  * example: key value caching or caching of aggregations results
  
**which caching design pattern is the most appropriate ?**

1. **lazy loading / cache aside / lazy population** : Cache hit, cache miss, write back to cache etc.
   * Pros:
       * only requested data is cached
       * cache isn't filled up with unused data
       * node failures are not fatal (increased latency to warm the cache)

   * Cons:
       * Cache miss penalty that results in 3 round trips, noticeable delay in requests

2. **Write through**: Add or Update cache when database is updated
    * Pros:
      * Data in cache is never stale
      * reads are quicker
      * write penalty vs read penalty (each write requires 2 calls by the app)
    * Cons:
      * Missing data until it is added / updated in the db. mitigation is to use lazy loading strategy as well. 


**Cache Evictions and TTL**
   * Cache evictions can happen in 3 ways
       * Delete them explicitly 
       * Item is evicted because memory is full and its not recently used (LRU)
       * You set an item time-to-live (or TTL)

   * TTL are helpful for any kind of data:
     * Leader boards
     * comments
     * activity streams
        * **allkeys-lfu**: The cache evicts the least frequently used (LFU) keys regardless of TTL set
        * **allkeys-lru**: The cache evicts the least recently used (LRU) regardless of TTL set
        * **volatile-lfu**: The cache evicts the least frequently used (LFU) keys from those that have a TTL set
        * **volatile-lru**: The cache evicts the least recently used (LRU) from those that have a TTL set
        * **volatile-ttl**: The cache evicts the keys with shortest TTL set
        * **volatile-random**: The cache randomly evicts keys with a TTL set
        * **allkeys-random**: The cache randomly evicts keys regardless of TTL set
        * **no-eviction**: The cache doesnâ€™t evict keys at all. This blocks future writes until memory frees up.

   * TTL can range from few seconds to hours or days

   * if too many evictions happen, scale up or out.
   * By default AWS sets the volatile-lru eviction policy to your Redis cluster 
  
**Thundering herd** : Also known as dog piling, when multiple apps hit the cache, get a cache miss and hit the db at the same time. More expensive the query, bigger the impact esp if the query is a top10 query.

  * example is millions of people following a star for update when their TTL expires and veryone queries the db at the same time (even if he/she hasn't published an update)
  * Also happens when a new cache node is added, pre-warm your node with a script and mimics what end users will do. 
  * If adding or removing clusters on a regular basis, can be automated with SNS whenever you app receives a cluster reconfiguration event
  * can be mitigated to some extent with the help of a randomised number added to a TTL for the session.  


  

 